{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本向量化流程\n",
    "* 文本标准化-字母转换位小写并删除标点符号、词干提取等\n",
    "* 词元化-单词级词元化、N元语法词元化、字符级词元化。如果关注词顺序的模型叫做序列模型，构建模型则使用单词级词元化；\n",
    "另外一种输入单词作为一个集合，不考虑原始顺序的叫词袋模型，构建模型则使用N元语法词元化；\n",
    "* 对所有词元建立索引-为词表中的每个单词分配唯一整数\n",
    "* 索引向量编码（one-hot编码或嵌入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载Imdb影评数据训练数据、验证数据、测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "train_ds=keras.utils.text_dataset_from_directory(\"../DL/data/aclImdb/train\",batch_size=batch_size)\n",
    "val_ds=keras.utils.text_dataset_from_directory(\"../DL/data/aclImdb/val\",batch_size=batch_size)\n",
    "test_ds=keras.utils.text_dataset_from_directory(\"../DL/data/aclImdb/test\",batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "tf.Tensor(b\"This movie doesn't even deserve a one. This was an utter waste of time. It was a waste of film and money. It was not offensive but everything was provocative and disgusting. My spoiler is one that I think should be read by everyone. There is full frontal nudity and disgusting language. But not only that, there is NO plot line, the actors are terrible, the accents are horrible, the actors are small time and I was even EXCITED to watch this movie! <br /><br />The only reason I rented it was for Brian van Holt (who got only a fifteen second part, by the way). I think this might have been a mistake on the directors and editors parts but they repeated the same segments two or three times, adding only a new sentence.<br /><br />A film similar to this is Eraser Head, possibly the most disturbing movie in existence. There is no plot line, and is not funny. Although it isn't trying to be funny. DO NOT WATCH EITHER MOVIE.\", shape=(), dtype=string)\n",
      "inputs shape: (32,)\n",
      "inputs dtype: <dtype: 'string'>\n",
      "targets shape: (32,)\n",
      "targets dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "for inputs,targets in train_ds:\n",
    "    print(inputs[0])\n",
    "    print(\"inputs shape:\",inputs.shape)\n",
    "    print(\"inputs dtype:\",inputs.dtype)\n",
    "    print(\"targets shape:\",targets.shape)\n",
    "    print(\"targets dtype:\",targets.dtype)\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用TextVectorization层预处理数据集\n",
    "> 对文本进行向量化流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']\n"
     ]
    }
   ],
   "source": [
    "#将词表限制为前20000个最常出现的单词。输出词元编码为multi-hot二进制编码,默认为一元语法词元化\n",
    "# text_vectorization=TextVectorization(max_tokens=20000,output_mode=\"multi_hot\") #一元语法词元化\n",
    "# text_vectorization=TextVectorization(max_tokens=20000,output_mode=\"multi_hot\",ngrams=2) #二元语法词元化\n",
    "# text_vectorization=TextVectorization(max_tokens=20000,output_mode=\"count\",ngrams=2) #二元语法的词频\n",
    "text_vectorization=TextVectorization(max_tokens=20000,output_mode=\"tf_idf\",ngrams=2) #二元语法的TF-IDF编码词元化\n",
    "#准备文本数据集\n",
    "text_only_train_ds=train_ds.map(lambda x,y:x)\n",
    "#对数据集词表建立索引\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "#显示词表,按照词出现的次数顺序展示。第一个是未知词\n",
    "print(text_vectorization.get_vocabulary()[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别对训练、验证、测试数据集进行multi-hot二进制编码处理，num_parallel_calls：并行处理\n",
    "tfidf_2gram_train_ds=train_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds=val_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds=test_ds.map(lambda x,y:(text_vectorization(x),y),num_parallel_calls=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "inputs shape: (32, 20000)\n",
      "inputs dtype: <dtype: 'float32'>\n",
      "targets shape: (32,)\n",
      "targets dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(\n",
      "[506.4039     10.459088    5.6893773 ...   0.          0.\n",
      "   0.       ], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_2gram_train_ds))\n",
    "for inputs,targets in tfidf_2gram_train_ds:\n",
    "    print(\"inputs shape:\",inputs.shape)\n",
    "    print(\"inputs dtype:\",inputs.dtype)\n",
    "    print(\"targets shape:\",targets.shape)\n",
    "    print(\"targets dtype:\",targets.dtype)\n",
    "    print(\"inputs[0]:\",inputs[0])\n",
    "    print(\"targets[0]:\",targets[0])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型构建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000,hidden_dim=16):\n",
    "    inputs=keras.Input(shape=(max_tokens,))\n",
    "    x=layers.Dense(hidden_dim,activation=\"relu\")(inputs)\n",
    "    x=layers.Dropout(0.5)(x)\n",
    "    outputs=layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "    model=keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 13s 19ms/step - loss: 0.4804 - accuracy: 0.7976 - val_loss: 0.2771 - val_accuracy: 0.9016\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.3130 - accuracy: 0.8777 - val_loss: 0.2583 - val_accuracy: 0.9060\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.2796 - accuracy: 0.8873 - val_loss: 0.2785 - val_accuracy: 0.9054\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.2555 - accuracy: 0.8966 - val_loss: 0.2742 - val_accuracy: 0.9064\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.2382 - accuracy: 0.9031 - val_loss: 0.2762 - val_accuracy: 0.9034\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2257 - accuracy: 0.9064 - val_loss: 0.2905 - val_accuracy: 0.8982\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2229 - accuracy: 0.9051 - val_loss: 0.3300 - val_accuracy: 0.8974\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.2228 - accuracy: 0.9081 - val_loss: 0.3020 - val_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2095 - accuracy: 0.9128 - val_loss: 0.3223 - val_accuracy: 0.9012\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2001 - accuracy: 0.9185 - val_loss: 0.3305 - val_accuracy: 0.9010\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 0.2831 - accuracy: 0.8921\n",
      "Test acc:0.892\n"
     ]
    }
   ],
   "source": [
    "model=get_model()\n",
    "model.summary()\n",
    "callbacks=[keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds,validation_data=tfidf_2gram_val_ds,epochs=10,callbacks=callbacks)\n",
    "model=keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc:{model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61df5942b7b0242e76337a89f6adf9064839c863323d0519cd2ba8ddb538740d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
